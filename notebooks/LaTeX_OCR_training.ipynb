{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtR1GhYwnLnu"
      },
      "source": [
        "# Train a LaTeX OCR model\n",
        "In this brief notebook I show how you can finetune/train an OCR model.\n",
        "\n",
        "I've opted to mix in handwritten data into the regular pdf LaTeX images. For that I started out with the released pretrained model and continued training on the slightly larger corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r396ah-Q3EQc"
      },
      "outputs": [],
      "source": [
        "!pip install pix2tex[train] -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ4PLwkb3RIs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "!mkdir -p LaTeX-OCR\n",
        "os.chdir('LaTeX-OCR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUsTlxXV3Mot"
      },
      "outputs": [],
      "source": [
        "!pip install gpustat -q\n",
        "!pip install opencv-python-headless==4.8.1.78 -U -q\n",
        "!pip install --upgrade --no-cache-dir gdown -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhLzh5vyaCaL"
      },
      "outputs": [],
      "source": [
        "# check what GPU we have\n",
        "!gpustat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAz37dDU21zu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from glob import glob\n",
        "\n",
        "# Create the necessary directories if they don't exist\n",
        "!mkdir -p dataset/data\n",
        "!mkdir -p dataset/data/images  # Ensure this directory is correctly referenced\n",
        "\n",
        "# Download the datasets using gdown\n",
        "!gdown -O dataset/data/HANDWRTN.zip --id 18-bFQV5m1Ir1pq8deQiAkw743KpBSBN3         # NEW DATA\n",
        "!gdown -O dataset/data/pdf.zip --id 176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ              # PDF IMAGES DATA\n",
        "!gdown -O dataset/data/pdfmath.txt --id 1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D          # PDF MATH DATA\n",
        "\n",
        "# Unzip the downloaded datasets\n",
        "!unzip -q dataset/data/HANDWRTN.zip -d dataset/data\n",
        "!unzip -q dataset/data/pdf.zip -d dataset/data\n",
        "\n",
        "# Define the number of validation images you want\n",
        "number_of_val_images = 1000\n",
        "\n",
        "# Change directory to where the images are expected to be after unzipping\n",
        "images_path = 'dataset/data/images'\n",
        "val_images_path = 'dataset/data/valimages'\n",
        "\n",
        "handwrtn_txt = '/content/LaTeX-OCR/dataset/data/HANDWRTN/HANDWRTN_math.txt'\n",
        "\n",
        "# Check if the 'images' directory exists and has files\n",
        "if os.path.exists(images_path) and os.path.isdir(images_path):\n",
        "    # Get all the image files in the 'images' directory\n",
        "    all_images = glob(os.path.join(images_path, '*'))\n",
        "\n",
        "    # Shuffle the list of images\n",
        "    random.shuffle(all_images)\n",
        "\n",
        "    # Ensure the validation directory exists\n",
        "    os.makedirs(val_images_path, exist_ok=True)\n",
        "\n",
        "    # Move a subset of images to the 'valimages' directory\n",
        "    for img in all_images[:number_of_val_images]:\n",
        "        shutil.move(img, val_images_path)\n",
        "\n",
        "    # The remaining files in 'images' are your training set\n",
        "else:\n",
        "    print(f\"The directory {images_path} does not exist or is not a directory.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BMuIqRIqG-8"
      },
      "source": [
        "Now we generate the datasets. We can string multiple datasets together to get one large lookup table. The only thing saved in these pkl files are image sizes, image location and the ground truth latex code. That way we can serve batches of images with the same dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JebcEarl-g6"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.dataset.dataset -i dataset/data/images dataset/data/train -e /content/LaTeX-OCR/dataset/data/HANDWRTN_math.txt dataset/data/pdfmath.txt -o dataset/data/train.pkl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modified where we pull our dataset from in our case **/content/LaTeX-OCR/dataset/data/HANDWRTN_math.txt** inside a Google Colab Runtime environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_Orutb37xHD"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.dataset.dataset -i dataset/data/valimages dataset/data/val -e /content/LaTeX-OCR/dataset/data/HANDWRTN_math.txt dataset/data/pdfmath.txt -o dataset/data/val.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3iOyEEBbw58"
      },
      "outputs": [],
      "source": [
        "# download the weights we want to fine tune\n",
        "!curl -L -o weights.pth https://github.com/lukas-blecher/LaTeX-OCR/releases/download/v0.0.1/weights.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vow2NnpHmWt0"
      },
      "outputs": [],
      "source": [
        "# If using wandb\n",
        "!pip install -q wandb \n",
        "# you can cancel this if you don't wan't to use it or don't have a W&B acc.\n",
        "#!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We modified batchsize, PAD, and epoch to train our handwritten data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnsNCLp84QSY"
      },
      "outputs": [],
      "source": [
        "# generate colab specific config (set 'debug' to true if wandb is not used)\n",
        "!echo {backbone_layers: [2, 3, 7], betas: [0.9, 0.999], batchsize: 15, bos_token: 1, channels: 1, data: dataset/data/train.pkl, debug: true, decoder_args: {'attn_on_attn': true, 'cross_attend': true, 'ff_glu': true, 'rel_pos_bias': false, 'use_scalenorm': false}, dim: 256, encoder_depth: 4, eos_token: 2, epochs: 5, gamma: 0.9995, heads: 8, id: null, load_chkpt: 'weights.pth', lr: 0.001, lr_step: 30, max_height: 192, max_seq_len: 512, max_width: 672, min_height: 32, min_width: 32, model_path: checkpoints, name: mixed, num_layers: 4, num_tokens: 8000, optimizer: Adam, output_path: outputs, pad: true, pad_token: 0, patch_size: 16, sample_freq: 2000, save_freq: 1, scheduler: StepLR, seed: 42, temperature: 0.2, test_samples: 5, testbatchsize: 20, tokenizer: dataset/tokenizer.json, valbatches: 100, valdata: dataset/data/val.pkl} > colab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8NU5j2k3z36"
      },
      "outputs": [],
      "source": [
        "!python -m pix2tex.train --config colab.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3DU9KxubWgq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "LaTeX-OCR training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
